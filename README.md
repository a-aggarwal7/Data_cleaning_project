# Data Cleaning Project

This project demonstrates data cleaning techniques applied to large datasets using SQL and Python. The goal was to improve data quality, consistency, and usability for downstream analysis.

## Key Tasks

* **Duplicate Handling**: Applied SQL functions to identify and remove duplicate records from large datasets.
* **Standardization**: Standardized inconsistent data formats (text, dates, numeric fields) to ensure consistency and accuracy.
* **Missing Values**: Handled missing or null values using imputation and filtering techniques.
* **Optimization**: Optimized datasets by removing irrelevant columns to improve efficiency for analysis.



## Tech Stack

* SQL (for querying and cleaning large datasets)



## How to Run

1. Clone the repository:

   ```bash
   git clone https://github.com/a-aggarwal7/Data_cleaning_project.git
   cd Data_cleaning_project
   ```
2. Open the SQL script provided.



## Results

The cleaned datasets are free of duplicates, standardized, and optimized, making them ready for further exploratory analysis and modeling.

